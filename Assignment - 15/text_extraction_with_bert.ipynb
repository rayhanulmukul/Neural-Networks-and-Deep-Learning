{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grPd4nemk_P0"
      },
      "source": [
        "# Text Extraction with BERT\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/23<br>\n",
        "**Last modified:** 2020/05/23<br>\n",
        "**Description:** Fine tune pretrained BERT from HuggingFace Transformers on SQuAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Da84dJk_P3"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This demonstration uses SQuAD (Stanford Question-Answering Dataset).\n",
        "In SQuAD, an input consists of a question, and a paragraph for context.\n",
        "The goal is to find the span of text in the paragraph that answers the question.\n",
        "We evaluate our performance on this data with the \"Exact Match\" metric,\n",
        "which measures the percentage of predictions that exactly match any one of the\n",
        "ground-truth answers.\n",
        "\n",
        "We fine-tune a BERT model to perform this task as follows:\n",
        "\n",
        "1. Feed the context and the question as inputs to BERT.\n",
        "2. Take two vectors S and T with dimensions equal to that of\n",
        "   hidden states in BERT.\n",
        "3. Compute the probability of each token being the start and end of\n",
        "   the answer span. The probability of a token being the start of\n",
        "   the answer is given by a dot product between S and the representation\n",
        "   of the token in the last layer of BERT, followed by a softmax over all tokens.\n",
        "   The probability of a token being the end of the answer is computed\n",
        "   similarly with the vector T.\n",
        "4. Fine-tune BERT and learn S and T along the way.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [BERT](https://arxiv.org/abs/1810.04805)\n",
        "- [SQuAD](https://arxiv.org/abs/1606.05250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikvoWKnWk_P4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x-Q4nc2lk_P4"
      },
      "outputs": [],
      "source": [
        "# ---------- 0) Env & Imports ----------\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # ensure tf-keras\n",
        "\n",
        "import re, json, string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tf_keras as keras\n",
        "from tf_keras import layers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "max_len = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTuRDAmck_P6"
      },
      "source": [
        "## Set-up BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aPaygw1Pk_P6"
      },
      "outputs": [],
      "source": [
        "# ---------- 2) Tokenizer (save slow -> load fast) ----------\n",
        "slow_tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "slow_tok.save_pretrained(save_path)\n",
        "tokenizer = BertWordPieceTokenizer(f\"{save_path}/vocab.txt\", lowercase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqNrJYAck_P7"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZqaciwtBk_P7"
      },
      "outputs": [],
      "source": [
        "# ---------- 3) Load SQuAD v1.1 ----------\n",
        "train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "eval_url  = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_url)\n",
        "eval_path  = keras.utils.get_file(\"eval.json\",  eval_url)\n",
        "\n",
        "with open(train_path) as f: raw_train = json.load(f)\n",
        "with open(eval_path)  as f: raw_eval  = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTljOVWQk_P8"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "1. Go through the JSON file and store every record as a `SquadExample` object.\n",
        "2. Go through each `SquadExample` and create `x_train, y_train, x_eval, y_eval`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MW_DAalIk_P8",
        "outputId": "cf4f1185-f4e8-4cf8-de81-50354fef5f77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87599 train examples, 10570 eval examples\n",
            "TPU not found/usable: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_cancellation_closes_chips=2, embedding_config=\"\", tpu_embedding_config=\"\", enable_whole_mesh_compilations=false, is_global_init=false, compilation_failure_closes_chips=false]\n",
            "Registered devices: [CPU, GPU]\n",
            "Registered kernels:\n",
            "  <no registered kernels>\n",
            "\n",
            "\t [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_68324]\n",
            "✅ Using single GPU\n"
          ]
        }
      ],
      "source": [
        "# ---------- 4) Example holder ----------\n",
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context  = \" \".join(str(self.context).split())\n",
        "        question = \" \".join(str(self.question).split())\n",
        "        answer   = \" \".join(str(self.answer_text).split())\n",
        "        end_char_idx = self.start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True; return\n",
        "\n",
        "        # mark answer chars\n",
        "        is_in_ans = [0]*len(context)\n",
        "        for i in range(self.start_char_idx, end_char_idx):\n",
        "            is_in_ans[i] = 1\n",
        "\n",
        "        # tokenize context & find token span\n",
        "        tok_ctx = tokenizer.encode(context)\n",
        "        ans_tok_idx = [i for i,(s,e) in enumerate(tok_ctx.offsets) if sum(is_in_ans[s:e])>0]\n",
        "        if not ans_tok_idx:\n",
        "            self.skip = True; return\n",
        "\n",
        "        start_token_idx = ans_tok_idx[0]\n",
        "        end_token_idx   = ans_tok_idx[-1]\n",
        "\n",
        "        # tokenize question\n",
        "        tok_q = tokenizer.encode(question)\n",
        "\n",
        "        # build [context tokens] + [question tokens w/o first special]\n",
        "        input_ids = tok_ctx.ids + tok_q.ids[1:]\n",
        "        token_type_ids = [0]*len(tok_ctx.ids) + [1]*len(tok_q.ids[1:])\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "\n",
        "        # pad / skip on overflow\n",
        "        pad = max_len - len(input_ids)\n",
        "        if pad < 0:\n",
        "            self.skip = True; return\n",
        "        if pad > 0:\n",
        "            input_ids      += [0]*pad\n",
        "            attention_mask += [0]*pad\n",
        "            token_type_ids += [0]*pad\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx   = end_token_idx\n",
        "        self.context_token_to_char = tok_ctx.offsets\n",
        "        self.context = context\n",
        "        self.all_answers = self.all_answers  # unchanged\n",
        "\n",
        "def build_examples(raw):\n",
        "    out = []\n",
        "    for item in raw[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            ctx = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                ans_text = qa[\"answers\"][0][\"text\"]\n",
        "                ans_all  = [a[\"text\"] for a in qa[\"answers\"]]\n",
        "                start    = qa[\"answers\"][0][\"answer_start\"]\n",
        "                ex = SquadExample(qa[\"question\"], ctx, start, ans_text, ans_all)\n",
        "                ex.preprocess()\n",
        "                out.append(ex)\n",
        "    return out\n",
        "\n",
        "def to_arrays(examples):\n",
        "    buf = {k: [] for k in [\"input_ids\",\"token_type_ids\",\"attention_mask\",\"start_token_idx\",\"end_token_idx\"]}\n",
        "    for e in examples:\n",
        "        if not e.skip:\n",
        "            buf[\"input_ids\"].append(e.input_ids)\n",
        "            buf[\"token_type_ids\"].append(e.token_type_ids)\n",
        "            buf[\"attention_mask\"].append(e.attention_mask)\n",
        "            buf[\"start_token_idx\"].append(e.start_token_idx)\n",
        "            buf[\"end_token_idx\"].append(e.end_token_idx)\n",
        "    for k in buf: buf[k] = np.array(buf[k])\n",
        "    x = [buf[\"input_ids\"], buf[\"token_type_ids\"], buf[\"attention_mask\"]]\n",
        "    y = [buf[\"start_token_idx\"], buf[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "train_examples = build_examples(raw_train)\n",
        "eval_examples  = build_examples(raw_eval)\n",
        "print(f\"{len(train_examples)} train examples, {len(eval_examples)} eval examples\")\n",
        "\n",
        "(x_ids_tr, x_seg_tr, x_mask_tr), (y_start_tr, y_end_tr) = to_arrays(train_examples)\n",
        "(x_ids_ev, x_seg_ev, x_mask_ev), (y_start_ev, y_end_ev) = to_arrays(eval_examples)\n",
        "\n",
        "# cast to int32 for TF\n",
        "x_ids_tr  = x_ids_tr.astype(\"int32\");  x_seg_tr  = x_seg_tr.astype(\"int32\");  x_mask_tr  = x_mask_tr.astype(\"int32\")\n",
        "x_ids_ev  = x_ids_ev.astype(\"int32\");  x_seg_ev  = x_seg_ev.astype(\"int32\");  x_mask_ev  = x_mask_ev.astype(\"int32\")\n",
        "y_start_tr = y_start_tr.astype(\"int32\"); y_end_tr = y_end_tr.astype(\"int32\")\n",
        "y_start_ev = y_start_ev.astype(\"int32\"); y_end_ev = y_end_ev.astype(\"int32\")\n",
        "\n",
        "x_train = {\"input_ids\": x_ids_tr, \"attention_mask\": x_mask_tr, \"token_type_ids\": x_seg_tr}\n",
        "y_train = [y_start_tr, y_end_tr]\n",
        "x_eval  = {\"input_ids\": x_ids_ev, \"attention_mask\": x_mask_ev, \"token_type_ids\": x_seg_ev}\n",
        "y_eval  = [y_start_ev, y_end_ev]\n",
        "\n",
        "# ---------- 5) Strategy ----------\n",
        "def get_strategy():\n",
        "    try:\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
        "        try: tf.config.experimental_connect_to_cluster(resolver)\n",
        "        except Exception: pass\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"✅ Using TPU\")\n",
        "        return tf.distribute.TPUStrategy(resolver)\n",
        "    except Exception as e:\n",
        "        print(f\"TPU not found/usable: {e}\")\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if len(gpus) > 1:\n",
        "        strat = tf.distribute.MirroredStrategy()\n",
        "        print(f\"✅ Using {strat.num_replicas_in_sync} GPUs (MirroredStrategy)\")\n",
        "        return strat\n",
        "    elif len(gpus) == 1:\n",
        "        print(\"✅ Using single GPU\")\n",
        "        return tf.distribute.OneDeviceStrategy(\"/GPU:0\")\n",
        "    else:\n",
        "        print(\"✅ Using CPU\")\n",
        "        return tf.distribute.OneDeviceStrategy(\"/CPU:0\")\n",
        "\n",
        "strategy = get_strategy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJJjYkInk_P9"
      },
      "source": [
        "Create the Question-Answering Model using BERT and Functional API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gd2piI1Kk_P9",
        "outputId": "940066df-5972-403a-f904-e320967eb875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"bert_qa\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf_bert_model_5 (TFBertMod  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
            " el)                         ngAndCrossAttentions(last_   40         'attention_mask[0][0]',      \n",
            "                             hidden_state=(None, 128, 7              'token_type_ids[0][0]']      \n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " start_logit (Dense)         (None, 128, 1)               768       ['tf_bert_model_5[0][0]']     \n",
            "                                                                                                  \n",
            " end_logit (Dense)           (None, 128, 1)               768       ['tf_bert_model_5[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)         (None, 128)                  0         ['start_logit[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)         (None, 128)                  0         ['end_logit[0][0]']           \n",
            "                                                                                                  \n",
            " start_probs (Activation)    (None, 128)                  0         ['flatten_4[0][0]']           \n",
            "                                                                                                  \n",
            " end_probs (Activation)      (None, 128)                  0         ['flatten_5[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109483776 (417.65 MB)\n",
            "Trainable params: 109483776 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# ---------- 6) Model ----------\n",
        "def create_model():\n",
        "    cfg = BertConfig.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False)\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", config=cfg, from_pt=True)\n",
        "\n",
        "    input_ids      = layers.Input((max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = layers.Input((max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
        "    token_type_ids = layers.Input((max_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
        "\n",
        "    enc = encoder(input_ids=input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  token_type_ids=token_type_ids)  # let Keras set training/inference\n",
        "    seq = enc.last_hidden_state  # (B, L, H)\n",
        "\n",
        "    start_logits = layers.Dense(1, use_bias=False, name=\"start_logit\")(seq)\n",
        "    end_logits   = layers.Dense(1, use_bias=False, name=\"end_logit\")(seq)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "    end_logits   = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(\"softmax\", name=\"start_probs\")(start_logits)\n",
        "    end_probs   = layers.Activation(\"softmax\",  name=\"end_probs\")(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, attention_mask, token_type_ids],\n",
        "        outputs=[start_probs, end_probs],\n",
        "        name=\"bert_qa\"\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    opt  = keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    model.compile(optimizer=opt, loss=[loss, loss], run_eagerly=False)\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "\n",
        "# warm-up to create variables before tf.function graph tracing\n",
        "_ = model(\n",
        "    {\"input_ids\": tf.zeros((1, max_len), tf.int32),\n",
        "     \"attention_mask\": tf.ones((1, max_len), tf.int32),\n",
        "     \"token_type_ids\": tf.zeros((1, max_len), tf.int32)},\n",
        "    training=False\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxYksjWgk_P-"
      },
      "source": [
        "This code should preferably be run on Google Colab TPU runtime.\n",
        "With Colab TPUs, each epoch will take 5-6 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Igb-Rfmk_P-"
      },
      "source": [
        "## Create evaluation Callback\n",
        "\n",
        "This callback will compute the exact match score using the validation data\n",
        "after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1Y0styiek_P-"
      },
      "outputs": [],
      "source": [
        "# ---------- 7) Exact Match callback ----------\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    text = re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    def __init__(self, x_eval, y_eval, eval_examples):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "        self.eval_examples = [e for e in eval_examples if not e.skip]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval, verbose=0)\n",
        "        count = 0\n",
        "        for i, (ps, pe) in enumerate(zip(pred_start, pred_end)):\n",
        "            e = self.eval_examples[i]\n",
        "            offsets = e.context_token_to_char\n",
        "            s = int(np.argmax(ps))\n",
        "            t = int(np.argmax(pe))\n",
        "            if s >= len(offsets): continue\n",
        "            cs = offsets[s][0]\n",
        "            if t < len(offsets):\n",
        "                ce = offsets[t][1]\n",
        "                pred = e.context[cs:ce]\n",
        "            else:\n",
        "                pred = e.context[cs:]\n",
        "            if normalize_text(pred) in [normalize_text(a) for a in e.all_answers]:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nEpoch {epoch+1}: Exact Match = {acc:.4f}\")\n",
        "\n",
        "exact_match_cb = ExactMatch(x_eval, y_eval, eval_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUt1vx7bk_P_"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lNNBRBErk_P_",
        "outputId": "a4d65268-60c8-4740-e275-f8d0f253569d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Exact Match = 0.7165\n",
            "2623/2623 - 706s - loss: 2.7384 - start_probs_loss: 1.4397 - end_probs_loss: 1.2987 - 706s/epoch - 269ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c8d95d424e0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# ---------- 8) Train ----------\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,              # increase to 2-3+ for better demo results\n",
        "    batch_size=16,\n",
        "    verbose=2,\n",
        "    callbacks=[exact_match_cb],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_extraction_with_bert",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}